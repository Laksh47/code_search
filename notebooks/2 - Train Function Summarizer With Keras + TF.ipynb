{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment is available as a publically available docker container: `hamelsmu/ml-gpu`\n",
    "\n",
    "### Pre-requisite: Familiarize yourself with sequence-to-sequence models\n",
    "\n",
    "If you are not familiar with sequence to sequence models, please refer to [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8).\n",
    "\n",
    "### Pre-Requisite: Make Sure you have the right files prepared from Step 1\n",
    "\n",
    "You should have these files in the root of the `./data/processed_data/` directory:\n",
    "\n",
    "1. `{train/valid/test.function}` - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2. `{train/valid/test.docstring}` - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3. `{train/valid/test.lineage}` - every line in this file contains a link back to the original location (github repo link) where the code was retrieved.  There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "\n",
    "### Set the value of `use_cache` appropriately.  \n",
    "\n",
    "If `use_cache = True`, data will be downloaded where possible instead of re-computing.  However, it is highly recommended that you set `use_cache = False` for this tutorial as it will be less confusing, and you will learn more by runing these steps yourself. **This notebook takes approximately 4 hours to run on an AWS `p3.8xlarge` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text From File\n",
    "\n",
    "We want to read in raw text from files so we can pre-process the text for modeling as described in [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for encoder holdout input: 177,220\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for decoder holdout input: 177,220\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, holdout_code, train_comment, holdout_comment = read_training_files('./data/processed_data/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593891\n",
      "593891\n"
     ]
    }
   ],
   "source": [
    "# #### Adding stackoverflow mined pairs with the training data\n",
    "# so_path = '/home/larumuga/Desktop/code-text-pairs/preprocessed_so/'\n",
    "# so_path = Path(so_path)\n",
    "\n",
    "# with open(so_path/'train.intent', 'r') as f:\n",
    "#     descriptions = f.readlines()\n",
    "\n",
    "# # train_comment = train_comment + descriptions\n",
    "# train_comment = descriptions\n",
    "    \n",
    "# with open(so_path/'train.snippets', 'r') as f:\n",
    "#     snippets = f.readlines()\n",
    "    \n",
    "# # train_code = train_code + snippets\n",
    "# train_code = snippets\n",
    "\n",
    "# print(len(train_code))\n",
    "# print(len(train_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "\n",
    "In this step, we are going to pre-process the raw text for modeling.  For an explanation of what this section does, see the [Preapre & Clean Data section of this Tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 10 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 34 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 3 sec\n",
      "WARNING:root:Finished parsing 593,891 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 6 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 10 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 30 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 3 sec\n",
      "WARNING:root:Finished parsing 593,891 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 6 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tokenized text** (You will reuse this for step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1227989, 55)\n",
      "Shape of decoder input: (1227989, 14)\n",
      "Shape of decoder target: (1227989, 14)\n",
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the above files on disk because you set `use_cache = True` you can download the files for the above function calls here:\n",
    "\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_code_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_comment_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code\n",
    "\n",
    "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1000,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for ./data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for ./data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n",
      "CPU times: user 12 s, sys: 1.41 s, total: 13.4 s\n",
      "Wall time: 9.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# if not use_cache:\n",
    "\n",
    "#     from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "#     import numpy as np\n",
    "#     from keras import optimizers\n",
    "\n",
    "#     seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#     script_name_base = 'py_func_sum_v9_'\n",
    "#     csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "\n",
    "#     model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "#                                        save_best_only=True)\n",
    "\n",
    "#     batch_size = 1000\n",
    "#     epochs = 16\n",
    "#     history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "#               batch_size=batch_size,\n",
    "#               epochs=epochs,\n",
    "#               validation_split=0.12, callbacks=[csv_logger, model_checkpoint])\n",
    "\n",
    "seq2seq_Model = load_model('./data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "num_encoder_tokens, enc_pp = load_text_processor('./data/seq2seq/py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor('./data/seq2seq/py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v9_.epoch16-val2.55276.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if `use_cache = True`.  \n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix `py_func_sum_v9_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Seq2Seq Model For Code Summarization\n",
    "\n",
    "To evaluate this model we are going to do two things:\n",
    "\n",
    "1.  Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "2.  Calculate the [BLEU Score](https://en.wikipedia.org/wiki/BLEU) so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1 =================\n",
      "\n",
      "Original Input:\n",
      " def get_field_event_unique_id self return uuid\n",
      " \n",
      "\n",
      "Original Output:\n",
      " field in the rich event with the unique i d\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return unique field unique i d\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 2 =================\n",
      "\n",
      "Original Input:\n",
      " def get_rich_events self item events module item data for release in item data releases event self get_rich_item item event uuid _ release slug event author_url https forge puppet com release module owner username event gravatar_id release module owner gravatar_id event downloads release downloads event slug release slug event version release version event uri release uri event validation_score release validation_score event homepage_url None if project_page in release metadata event homepage_url release metadata project_page event issues_url None if issues_url in release metadata event issues_url release metadata issues_url event tags release tags event license release metadata license event source_url release metadata source event summary release metadata summary event metadata__updated_on parser parse release updated_at isoformat if self sortinghat release metadata__updated_on event metadata__updated_on event update self get_item_sh release if self prjs_map event update self get_item_project event event update self get_grimoire_fields release created_at release events append event return events\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get the enriched events related to a module\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the rich data for the item\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 3 =================\n",
      "\n",
      "Original Input:\n",
      " def get_sh_identity self item identity_field None identity for field in name email username identity field None if item is None return identity user item if data in item and type item dict user item data fields identity_field if user is None return identity if displayName in user identity name user displayName if name in user identity username user name if emailAddress in user identity email user emailAddress return identity\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return a sorting hat identity using jira user data\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the identity of the identity field for the given identity\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 4 =================\n",
      "\n",
      "Original Input:\n",
      " def get_users_data self item if data in item users_data item data fields else users_data item return users_data\n",
      " \n",
      "\n",
      "Original Output:\n",
      " if user fields are inside the global item dict\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the data for the user\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities item item data for field in assignee reporter creator if field not in item fields continue if item fields field identities append self get_sh_identity item fields field return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of identities for a given identities\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 6 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def fix_value_null value if value null return None else return value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " fix < null > values in some jira parameters .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " if value is none return none\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 7 =================\n",
      "\n",
      "Original Input:\n",
      " classmethod def enrich_fields cls fields eitem for field in fields if field startswith customfield_ if type fields field is dict if name in fields field if fields field name Story Points eitem story_points fields field value elif fields field name Sprint value fields field value if value sprint value 0 partition name 2 split 0 sprint_start value 0 partition startDate 2 split 0 sprint_end value 0 partition endDate 2 split 0 sprint_complete value 0 partition completeDate 2 split 0 eitem sprint sprint eitem sprint_start cls fix_value_null sprint_start eitem sprint_end cls fix_value_null sprint_end eitem sprint_complete cls fix_value_null sprint_complete\n",
      " \n",
      "\n",
      "Original Output:\n",
      " enrich the fields property of an issue .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " enrich the fields in the fields of the given fields\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 8 =================\n",
      "\n",
      "Original Input:\n",
      " def test_items_to_raw self result self _test_items_to_raw self assertEqual result items 3 self assertEqual result raw 3\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test whether json items are properly inserted into es\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that the raw result is returned\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 9 =================\n",
      "\n",
      "Original Input:\n",
      " def test_raw_to_enrich self result self _test_raw_to_enrich self assertEqual result raw 3 self assertEqual result enrich 6\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test whether the raw index is properly enriched\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test the conversion of a raw string to a simple string\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 10 =================\n",
      "\n",
      "Original Input:\n",
      " def test_raw_to_enrich_sorting_hat self result self _test_raw_to_enrich sortinghat True self assertEqual result raw 3 self assertEqual result enrich 6\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test enrich with sortinghat\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test conversion from a list of strings to a simple list of strings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 11 =================\n",
      "\n",
      "Original Input:\n",
      " def test_raw_to_enrich_projects self result self _test_raw_to_enrich projects True self assertEqual result raw 3 self assertEqual result enrich 6\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test enrich with projects\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test projects to projects with a project\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 12 =================\n",
      "\n",
      "Original Input:\n",
      " def test_refresh_identities self result self _test_refresh_identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test refresh identities\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " refresh the identities list\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 13 =================\n",
      "\n",
      "Original Input:\n",
      " def test_refresh_project self result self _test_refresh_project\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test refresh project field for all sources\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " refresh the project\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 14 =================\n",
      "\n",
      "Original Input:\n",
      " def test_arthur_params self with open data projects release json as projects_filename url json load projects_filename grimoire stackexchange 0 arthur_params site stackoverflow com tagged ovirt self assertDictEqual arthur_params StackExchangeOcean get_arthur_params_from_url url\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test the extraction of arthur params from an url\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test the loading of a project\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 15 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping properties fullDisplayName_analyzed type text index true else mapping properties fullDisplayName_analyzed type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic mappings from elasticsearch\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 16 =================\n",
      "\n",
      "Original Input:\n",
      " def set_jenkins_rename_file self nodes_rename_file self nodes_rename_file nodes_rename_file self __load_node_renames logger info Jenkis node rename file active s nodes_rename_file\n",
      " \n",
      "\n",
      "Original Output:\n",
      " file with nodes renaming mapping :\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " sets the file name of the jenkins node\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 17 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of identities\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 18 =================\n",
      "\n",
      "Original Input:\n",
      " def get_fields_from_job_name self job_name extra_fields category None installer None scenario None testproject None pod None loop None branch None try components job_name split if len components 2 return extra_fields kind components 1 if kind os extra_fields category parent main extra_fields installer components 0 extra_fields scenario join components 2 3 elif kind deploy extra_fields category deploy extra_fields installer components 0 else extra_fields category test extra_fields testproject components 0 extra_fields installer components 1 extra_fields pod components 3 extra_fields loop components 2 extra_fields branch components 1 except IndexError as ex logger debug Problems parsing job name s job_name logger debug ex return extra_fields\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"analyze a jenkins job name , producing a dictionary\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a job definition from a job definition\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 19 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping properties text_analyzed type text fielddata true else mapping properties text_analyzed type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 20 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities message item data message identity self get_sh_identity message from identities append identity return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of identities\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 21 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties body_markdown type text index true answers properties body_markdown type text index true else mapping dynamic true properties data properties body_markdown type string index analyzed answers properties body_markdown type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search parameters\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 22 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping properties title_analyzed type text else mapping properties title_analyzed type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " extract elastic search mappings from elasticsearch\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 23 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities if data not in item return identities if revisions not in item data return identities revisions item data revisions for revision in revisions user self get_sh_identity revision identities append user return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of identities for the given identities\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 24 =================\n",
      "\n",
      "Original Input:\n",
      " def get_review_sh self revision item identity self get_sh_identity revision update parser parse item self get_field_date erevision self get_item_sh_fields identity update return erevision\n",
      " \n",
      "\n",
      "Original Output:\n",
      " add sorting hat enrichment fields for the author of the revision\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get the review branch for the given revision\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 25 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties answers dynamic false properties author dynamic false properties comments dynamic false properties summary type text index true else mapping dynamic true properties data properties answers dynamic false properties author dynamic false properties comments dynamic false properties summary type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search parameters\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26 =================\n",
      "\n",
      "Original Input:\n",
      " def check_params params check True for f in params_fields if f not in params logging info Param not received s f check False break return check\n",
      " \n",
      "\n",
      "Original Output:\n",
      " check if we have all params to create the dashboard\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " check parameters for validity\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 27 =================\n",
      "\n",
      "Original Input:\n",
      " app route api dashboard methods POST def dashboard if request headers Content Type application json params request json print request json if not check_params params error Error in params Please include s join params_fields msg params json dumps params error error resp Response json dumps msg status 400 mimetype application json return resp else url build_dashboard params msg json dumps url url resp Response msg status 200 mimetype application json return resp\n",
      " \n",
      "\n",
      "Original Output:\n",
      " create a new dashboard using params from post\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " dashboard endpoint\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 28 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties revisions properties comment type text index true else mapping dynamic true properties data properties revisions properties comment type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 29 =================\n",
      "\n",
      "Original Input:\n",
      " def test_raw_to_enrich self result self _test_raw_to_enrich self assertEqual result raw 3 self assertEqual result enrich 3\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test whether the raw index is properly enriched\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test the conversion of a raw string to a simple string\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 30 =================\n",
      "\n",
      "Original Input:\n",
      " def test_raw_to_enrich_sorting_hat self result self _test_raw_to_enrich sortinghat True self assertEqual result raw 3 self assertEqual result enrich 3\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test enrich with sortinghat\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test conversion from a list of strings to a simple list of strings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 31 =================\n",
      "\n",
      "Original Input:\n",
      " def test_raw_to_enrich_projects self result self _test_raw_to_enrich projects True self assertEqual result raw 3 self assertEqual result enrich 3\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test enrich with projects\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test projects to projects with a project\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 32 =================\n",
      "\n",
      "Original Input:\n",
      " def test_arthur_params self with open data projects release json as projects_filename url json load projects_filename grimoire hyperkitty 0 arthur_params uri https lists mailman3 org archives list mailman users mailman3 org url https lists mailman3 org archives list mailman users mailman3 org self assertDictEqual arthur_params HyperKittyOcean get_arthur_params_from_url url\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test the extraction of arthur params from an url\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that the params are loaded correctly\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 33 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties description type text index true else mapping dynamic true properties data properties description type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 34 =================\n",
      "\n",
      "Original Input:\n",
      " def get_params_parser parser argparse ArgumentParser parser add_argument e elastic_url default http 127 0 0 1 9200 help Host with elastic search default http 127 0 0 1 9200 parser add_argument g debug dest debug action store_true parser add_argument t token dest token help GitHub token parser add_argument o org dest org nargs help GitHub Organization s to be analyzed parser add_argument l list dest list action store_true help Just list the repositories parser add_argument n nrepos dest nrepos type int default NREPOS help Number of GitHub repositories from the Organization to be analyzed default 0 no limit parser add_argument db projects map help Database to include the projects Mapping DB return parser\n",
      " \n",
      "\n",
      "Original Output:\n",
      " parse command line arguments\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " parse command line arguments\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 35 =================\n",
      "\n",
      "Original Input:\n",
      " def get_owner_repos_url owner token url_org GITHUB_API_URL orgs owner repos url_user GITHUB_API_URL users owner repos url_owner url_org try r requests get url_org params get_payload headers get_headers token r raise_for_status except requests exceptions HTTPError as e if r status_code 403 rate_limit_reset_ts datetime fromtimestamp int r headers X RateLimit Reset seconds_to_reset rate_limit_reset_ts datetime utcnow seconds 1 logging info GitHub rate limit exhausted Waiting i secs for rate limit reset seconds_to_reset sleep seconds_to_reset else url_owner url_user return url_owner\n",
      " \n",
      "\n",
      "Original Output:\n",
      " the owner could be a org or a user . it waits if need to have rate limit . also it fixes a djando issue changing - with _\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get the owner of the repo\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36 =================\n",
      "\n",
      "Original Input:\n",
      " def get_repositores owner_url token nrepos all_repos url owner_url while True logging debug Getting repos from s url try r requests get url params get_payload headers get_headers token r raise_for_status all_repos r json logging debug Rate limit s r headers X RateLimit Remaining if next not in r links break url r links next url except requests exceptions ConnectionError logging error Can not connect to GitHub break nrepos_recent repo for repo in all_repos if not repo fork nrepos_sorted sorted nrepos_recent key lambda repo parser parse repo updated_at reverse True if nrepos 0 nrepos_sorted nrepos_sorted 0 nrepos nrepos_sorted sorted nrepos_sorted key lambda repo repo size for repo in nrepos_sorted logging debug s i s repo updated_at repo size repo name return nrepos_sorted\n",
      " \n",
      "\n",
      "Original Output:\n",
      " owner could be an org or and user\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Predicted Output ******:\n",
      " get the owner of the given repository\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 37 =================\n",
      "\n",
      "Original Input:\n",
      " def test_enrich_fields self for ritem eitem in zip self ritems self eitems enriched if fields in ritem _source data JiraEnrich enrich_fields ritem _source data fields enriched self assertDictEqual enriched eitem\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test enrich_fields function\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that fields can be added to the fields\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 38 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping properties description_analyzed type text else mapping properties description_analyzed type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic mappings from elasticsearch\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 39 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities field self get_field_author identities append self get_sh_identity item field return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get identities for a specific field\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 40 =================\n",
      "\n",
      "Original Input:\n",
      " def get_rich_events self item events if version_downloads_data not in item data return events eitem self get_rich_item item for sample in item data version_downloads_data version_downloads event deepcopy eitem event download_sample_id sample id event sample_date sample date sample_date parser parse event sample_date event sample_version sample version event sample_downloads sample downloads event update self get_grimoire_fields sample_date isoformat downloads_event events append event return events\n",
      " \n",
      "\n",
      "Original Output:\n",
      " in the events there are some common fields with the crate . the name of the field must be the same in the create and in the downloads event so we can filer using it in crate and event at the same time .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a dict of events for the given item\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 41 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties journals dynamic false properties subject type text index true description type text index true else mapping dynamic true properties data properties journals dynamic false properties subject type string index analyzed description type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 42 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties message type text index true else mapping dynamic true properties data properties message type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 43 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping properties description_analyzed type text index true comment type text index true venue_geolocation type geo_point group_geolocation type geo_point else mapping properties description_analyzed type string index analyzed comment type string index analyzed venue_geolocation type geo_point group_geolocation type geo_point return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 44 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities item item data identities if event_hosts in item user self get_sh_identity item event_hosts 0 identities append user for rsvp in item rsvps user self get_sh_identity rsvp member identities append user for comment in item comments user self get_sh_identity comment member identities append user return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get identities for a given identities\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 45 =================\n",
      "\n",
      "Original Input:\n",
      " def get_item_sh self item sh_fields if member in item identity self get_sh_identity item member elif event_hosts in item identity self get_sh_identity item event_hosts 0 else return sh_fields created unixtime_to_datetime item created 1000 sh_fields self get_item_sh_fields identity created return sh_fields\n",
      " \n",
      "\n",
      "Original Output:\n",
      " add sorting hat enrichment fields\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get item s i d from the item\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 46 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping properties Subject_analyzed type text fielddata true body type text else mapping properties Subject_analyzed type string index analyzed body type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " extract elasticsearch mappings from elasticsearch\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 47 =================\n",
      "\n",
      "Original Input:\n",
      " def get_identities self item identities item item data for identity in From if identity in item and item identity user self get_sh_identity item identity identities append user return identities\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return the identities from an item\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return identities for a given identities\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 48 =================\n",
      "\n",
      "Original Input:\n",
      " def test_check_instance self major ElasticSearch _check_instance self url_es5 False self assertEqual major 5 major ElasticSearch _check_instance self url_es6 False self assertEqual major 6 with self assertRaises ElasticConnectException major ElasticSearch _check_instance self url_es6_err False\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test _ check_instance function\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test instance check instance\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 49 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major mapping dynamic true properties data properties versions_data dynamic false properties return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic mappings\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 50 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def get_elastic_mappings es_major if es_major 2 mapping dynamic true properties data properties renderedFields dynamic false properties fields properties description type text index true else mapping dynamic true properties data properties renderedFields dynamic false properties fields properties description type string index analyzed return items mapping\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get elasticsearch mapping .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get elastic search mappings\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd\n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on manual inspection of results:\n",
    "\n",
    "The predicted code summaries are not perfect, but we can see that the model has learned to extract some semantic meaning from the code.  That's all we need to get reasonable results in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score (on holdout set)\n",
    "\n",
    "BLEU Score is described [in this wikipedia article](https://en.wikipedia.org/wiki/BLEU), and is a way to measure the efficacy of summarization/translation such as the one we conducted here.  This metric is useful if you wish to conduct extensive hyper-parameter tuning and try to improve the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e34181c8f1e484dbaa26983e2d71bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0013320478813923105\n"
     ]
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "bleu_score = seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk\n",
    "\n",
    "Save the model to disk so you can use it in Step 4 of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')\n",
    "seq2seq_Model.save('./data/seq2seq/code_summary_seq2seq_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
